cache_file_h5py: '../file_data/word_a_a_userdict/math_data.h5'
cache_file_pickle: '../file_data/word_a_a_userdict/vocab_label.pkl'
embeddings: '../file_data/word_a_a_userdict/embeddings.pkl'
maxlen: 100 # 句子最大长度
emb_size: 300
epochs: 40
batch_size: 512 # 批处理尺寸, 感觉原则上越大越好,尤其是样本不均衡的时候, batch_size设置影响比较大
alpha: 4 # new model 的 loss 中的 alpha
hidden_size: 512 # lstm
lcm_stop: 50
